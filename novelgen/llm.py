"""
LLMå®ä¾‹ç®¡ç†
ç»Ÿä¸€ç®¡ç†LLMå®ä¾‹çš„åˆ›å»º
"""
import time
import json
from typing import Any, Dict, List, Type, TypeVar
from pydantic import BaseModel
from langchain_openai import ChatOpenAI
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.outputs import LLMResult
from langchain_core.messages import BaseMessage
from novelgen.config import LLMConfig

T = TypeVar("T", bound=BaseModel)


class VerboseCallbackHandler(BaseCallbackHandler):
    """
    è¯¦ç»†æ—¥å¿—å›è°ƒå¤„ç†å™¨
    
    æ”¯æŒæµå¼è¾“å‡º LLM å“åº”å†…å®¹åˆ°æ§åˆ¶å°å’Œ/æˆ–æ—¥å¿—æ–‡ä»¶ã€‚
    
    Args:
        stream_to_file: å¯é€‰çš„æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæä¾›åˆ™åŒæ—¶å†™å…¥æ–‡ä»¶
        show_streaming: æ˜¯å¦å®æ—¶æ˜¾ç¤ºæµå¼è¾“å‡ºï¼ˆé»˜è®¤ Trueï¼‰
        show_prompt: æ˜¯å¦æ˜¾ç¤ºå®Œæ•´çš„æç¤ºè¯ï¼ˆé»˜è®¤ Trueï¼‰
    """
    
    def __init__(self, stream_to_file: str = None, show_streaming: bool = True, show_prompt: bool = True):
        self.start_time = None
        self.end_time = None
        self.prompts = []
        self.total_tokens = 0
        self.prompt_tokens = 0
        self.completion_tokens = 0
        # æ–°å¢ï¼šå­˜å‚¨å®Œæ•´å“åº”å†…å®¹
        self.response_content = ""
        # æ–°å¢ï¼šæ˜¯å¦å®æ—¶æ˜¾ç¤ºæµå¼è¾“å‡º
        self.show_streaming = show_streaming
        # æ–°å¢ï¼šæ˜¯å¦æ˜¾ç¤ºæç¤ºè¯
        self.show_prompt = show_prompt
        # æ–°å¢ï¼šå¯é€‰è¾“å‡ºåˆ°æ–‡ä»¶
        self.stream_file_path = stream_to_file
        self.stream_file = None
        if stream_to_file:
            try:
                self.stream_file = open(stream_to_file, 'a', encoding='utf-8')
            except Exception as e:
                print(f"âš ï¸ æ— æ³•æ‰“å¼€æ—¥å¿—æ–‡ä»¶ {stream_to_file}: {e}")
    
    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs) -> None:
        """LLMå¼€å§‹è°ƒç”¨æ—¶"""
        self.start_time = time.time()
        self.prompts = prompts
        self.response_content = ""  # é‡ç½®å“åº”å†…å®¹
        
        print("\n" + "="*80)
        print("ğŸ¤– LLMè°ƒç”¨å¼€å§‹")
        print("="*80)
        
        if self.show_prompt:
            print("\nğŸ“ å®Œæ•´æç¤ºè¯ï¼š")
            print("-"*80)
            for i, prompt in enumerate(prompts, 1):
                print(f"\n[æç¤ºè¯ {i}]")
                print(prompt)
                print("-"*80)
        
        if self.show_streaming:
            print("\nğŸ“¤ LLM å“åº”ï¼ˆæµå¼è¾“å‡ºï¼‰ï¼š")
            print("-"*80)
    
    def on_chat_model_start(self, serialized: Dict[str, Any], messages: List[List[BaseMessage]], **kwargs) -> None:
        """èŠå¤©æ¨¡å‹å¼€å§‹è°ƒç”¨æ—¶"""
        self.start_time = time.time()
        self.response_content = ""  # é‡ç½®å“åº”å†…å®¹
        
        print("\n" + "="*80)
        print("ğŸ¤– LLMè°ƒç”¨å¼€å§‹")
        print("="*80)
        
        if self.show_prompt:
            print("\nğŸ“ å®Œæ•´æç¤ºè¯ï¼š")
            print("-"*80)
            
            for i, message_list in enumerate(messages, 1):
                print(f"\n[å¯¹è¯ {i}]")
                for msg in message_list:
                    role = msg.__class__.__name__.replace("Message", "")
                    content = msg.content
                    print(f"\n[{role}]")
                    print(content)
                print("-"*80)
        
        if self.show_streaming:
            print("\nğŸ“¤ LLM å“åº”ï¼ˆæµå¼è¾“å‡ºï¼‰ï¼š")
            print("-"*80)
    
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        """æµå¼è¾“å‡ºæ¯ä¸ª token"""
        self.response_content += token
        # å®æ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
        if self.show_streaming:
            print(token, end="", flush=True)
        # å¯é€‰ï¼šå†™å…¥æ—¥å¿—æ–‡ä»¶
        if self.stream_file:
            try:
                self.stream_file.write(token)
                self.stream_file.flush()
            except Exception:
                pass  # å¿½ç•¥å†™å…¥é”™è¯¯
    
    def on_llm_end(self, response: LLMResult, **kwargs) -> None:
        """LLMè°ƒç”¨ç»“æŸæ—¶"""
        self.end_time = time.time()
        elapsed_time = self.end_time - self.start_time if self.start_time else 0
        
        # æå–tokenä½¿ç”¨æƒ…å†µ
        if response.llm_output and 'token_usage' in response.llm_output:
            token_usage = response.llm_output['token_usage']
            self.total_tokens = token_usage.get('total_tokens', 0)
            self.prompt_tokens = token_usage.get('prompt_tokens', 0)
            self.completion_tokens = token_usage.get('completion_tokens', 0)
        
        # å¦‚æœæ²¡æœ‰æµå¼è¾“å‡ºï¼ˆresponse_content ä¸ºç©ºï¼‰ï¼Œä» response ä¸­æå–å†…å®¹
        if not self.response_content and response.generations:
            for gen_list in response.generations:
                for gen in gen_list:
                    if hasattr(gen, 'text'):
                        self.response_content += gen.text
                    elif hasattr(gen, 'message') and hasattr(gen.message, 'content'):
                        self.response_content += gen.message.content
        
        if self.show_streaming:
            print("\n" + "-"*80)
        
        print("\n" + "="*80)
        print("âœ… LLMè°ƒç”¨å®Œæˆ")
        print("="*80)
        print(f"\nâ±ï¸  å“åº”æ—¶é—´: {elapsed_time:.2f} ç§’")
        print(f"\nğŸ¯ Tokenä½¿ç”¨æƒ…å†µ:")
        print(f"  â€¢ æç¤ºè¯Token: {self.prompt_tokens}")
        print(f"  â€¢ ç”ŸæˆToken: {self.completion_tokens}")
        print(f"  â€¢ æ€»è®¡Token: {self.total_tokens}")
        
        # æ˜¾ç¤ºå®Œæ•´å“åº”å†…å®¹ï¼ˆæ–¹ä¾¿è°ƒè¯• JSON è§£æé—®é¢˜ï¼‰
        if self.response_content and not self.show_streaming:
            # åªæœ‰åœ¨éæµå¼æ¨¡å¼ä¸‹æ‰æ˜¾ç¤ºå®Œæ•´å“åº”ï¼ˆæµå¼æ¨¡å¼å·²ç»æ˜¾ç¤ºè¿‡äº†ï¼‰
            print(f"\nğŸ“„ å®Œæ•´å“åº”å†…å®¹ï¼š")
            print("-"*80)
            print(self.response_content)
            print("-"*80)
        
        print("\n" + "="*80 + "\n")
        
        # å†™å…¥åˆ†éš”ç¬¦åˆ°æ—¥å¿—æ–‡ä»¶
        if self.stream_file:
            try:
                self.stream_file.write(f"\n{'='*80}\n")
                self.stream_file.write(f"å“åº”æ—¶é—´: {elapsed_time:.2f}s, Tokens: {self.total_tokens}\n")
                self.stream_file.write(f"{'='*80}\n\n")
                self.stream_file.flush()
            except Exception:
                pass
    
    def on_llm_error(self, error: Exception, **kwargs) -> None:
        """LLMè°ƒç”¨å‡ºé”™æ—¶"""
        self.end_time = time.time()
        elapsed_time = self.end_time - self.start_time if self.start_time else 0
        
        print("\n" + "="*80)
        print("âŒ LLMè°ƒç”¨å‡ºé”™")
        print("="*80)
        print(f"\nâ±ï¸  å·²è€—æ—¶: {elapsed_time:.2f} ç§’")
        print(f"\nâ— é”™è¯¯ä¿¡æ¯: {error}")
        
        # å¦‚æœæœ‰éƒ¨åˆ†å“åº”å†…å®¹ï¼Œä¹Ÿæ˜¾ç¤ºå‡ºæ¥å¸®åŠ©è°ƒè¯•
        if self.response_content:
            print(f"\nğŸ“„ å·²æ¥æ”¶çš„éƒ¨åˆ†å“åº”ï¼š")
            print("-"*80)
            print(self.response_content)
            print("-"*80)
        
        print("\n" + "="*80 + "\n")
        
        # å†™å…¥é”™è¯¯ä¿¡æ¯åˆ°æ—¥å¿—æ–‡ä»¶
        if self.stream_file:
            try:
                self.stream_file.write(f"\n{'='*80}\n")
                self.stream_file.write(f"âŒ é”™è¯¯: {error}\n")
                self.stream_file.write(f"{'='*80}\n\n")
                self.stream_file.flush()
            except Exception:
                pass
    
    def __del__(self):
        """æ¸…ç†æ–‡ä»¶å¥æŸ„"""
        if self.stream_file:
            try:
                self.stream_file.close()
            except Exception:
                pass


def get_llm(
    config: LLMConfig = None, 
    verbose: bool = False, 
    stream_to_file: str = None,
    show_streaming: bool = True,
    show_prompt: bool = True
):
    """
    è·å–LLMå®ä¾‹
    
    Args:
        config: LLMé…ç½®ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        verbose: æ˜¯å¦å¯ç”¨è¯¦ç»†æ—¥å¿—è¾“å‡ºï¼ˆåŒæ—¶å¯ç”¨æµå¼è¾“å‡ºï¼‰
        stream_to_file: å¯é€‰çš„æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼Œå°† LLM å“åº”å†™å…¥æ–‡ä»¶
        show_streaming: æ˜¯å¦åœ¨æ§åˆ¶å°å®æ—¶æ˜¾ç¤ºæµå¼è¾“å‡ºï¼ˆé»˜è®¤ Trueï¼‰
        show_prompt: æ˜¯å¦åœ¨ verbose æ¨¡å¼ä¸‹æ˜¾ç¤ºå®Œæ•´æç¤ºè¯ï¼ˆé»˜è®¤ Trueï¼‰
        
    Returns:
        ChatOpenAIå®ä¾‹
    """
    if config is None:
        config = LLMConfig()
    
    callbacks = []
    if verbose:
        callbacks.append(VerboseCallbackHandler(
            stream_to_file=stream_to_file,
            show_streaming=show_streaming,
            show_prompt=show_prompt
        ))
    
    extra_body = None
    # ä¸ºé˜¿é‡Œäº‘ModelScopeçš„æ‰€æœ‰Qwenæ¨¡å‹è®¾ç½®enable_thinking=False
    if config.base_url and "api-inference.modelscope.cn" in config.base_url:
        import re
        # åŒ¹é…æ‰€æœ‰Qwenç³»åˆ—æ¨¡å‹
        qwen_patterns = [
            r"qwen.*",  # é€šç”¨qwenæ¨¡å‹
            r"Qwen.*",  # å¤§å†™å¼€å¤´çš„Qwenæ¨¡å‹
            r"Qwen3-\d+B",  # åŸæœ‰çš„Qwen3æ¨¡å‹
        ]
        if any(re.search(pattern, config.model_name or "", re.IGNORECASE) for pattern in qwen_patterns):
            extra_body = {"enable_thinking": False}
    
    return ChatOpenAI(
        model=config.model_name,
        temperature=config.temperature,
        max_tokens=config.max_tokens,
        api_key=config.api_key,
        base_url=config.base_url,
        callbacks=callbacks if callbacks else None,
        extra_body=extra_body,
        streaming=verbose  # å½“ verbose=True æ—¶å¯ç”¨æµå¼ä¼ è¾“
    )


def get_structured_llm(
    pydantic_model: Type[T], 
    config: LLMConfig = None, 
    verbose: bool = False,
    stream_to_file: str = None,
    show_streaming: bool = True,
    show_prompt: bool = True
):
    """
    è·å–æ”¯æŒç»“æ„åŒ–è¾“å‡ºçš„LLMå®ä¾‹
    
    Args:
        pydantic_model: Pydanticæ¨¡å‹ç±»ï¼Œç”¨äºå®šä¹‰è¾“å‡ºç»“æ„
        config: LLMé…ç½®ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        verbose: æ˜¯å¦å¯ç”¨è¯¦ç»†æ—¥å¿—è¾“å‡ºï¼ˆåŒæ—¶å¯ç”¨æµå¼è¾“å‡ºï¼‰
        stream_to_file: å¯é€‰çš„æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼Œå°† LLM å“åº”å†™å…¥æ–‡ä»¶
        show_streaming: æ˜¯å¦åœ¨æ§åˆ¶å°å®æ—¶æ˜¾ç¤ºæµå¼è¾“å‡ºï¼ˆé»˜è®¤ Trueï¼‰
        show_prompt: æ˜¯å¦åœ¨ verbose æ¨¡å¼ä¸‹æ˜¾ç¤ºå®Œæ•´æç¤ºè¯ï¼ˆé»˜è®¤ Trueï¼‰
        
    Returns:
        é…ç½®äº† with_structured_output çš„ ChatOpenAI å®ä¾‹
        
    Note:
        - å¦‚æœ config.use_structured_output ä¸º Falseï¼Œåˆ™è¿”å›æ™®é€š LLM å®ä¾‹ï¼ˆä¸ä½¿ç”¨ structured_outputï¼‰
        - å¦‚æœåç«¯ä¸æ”¯æŒ structured_outputï¼ˆå¦‚éƒ¨åˆ†å…¼å®¹ç«¯ç‚¹ï¼‰ï¼Œåº”åœ¨è°ƒç”¨ä¾§æ•è·å¼‚å¸¸å¹¶é€€å›åˆ°ä¼ ç»Ÿè§£æè·¯å¾„
    """
    if config is None:
        config = LLMConfig()
    
    # å¦‚æœé…ç½®æ˜ç¡®ç¦ç”¨ structured_outputï¼Œè¿”å›æ™®é€š LLM
    if not config.use_structured_output:
        return get_llm(
            config=config, 
            verbose=verbose,
            stream_to_file=stream_to_file,
            show_streaming=show_streaming,
            show_prompt=show_prompt
        )
    
    # åˆ›å»ºåŸºç¡€ LLM å®ä¾‹
    base_llm = get_llm(
        config=config, 
        verbose=verbose,
        stream_to_file=stream_to_file,
        show_streaming=show_streaming,
        show_prompt=show_prompt
    )
    
    # ä½¿ç”¨ with_structured_output åŒ…è£…
    try:
        return base_llm.with_structured_output(pydantic_model)
    except Exception as e:
        # å¦‚æœ with_structured_output ä¸è¢«æ”¯æŒï¼Œæ‰“å°è­¦å‘Šå¹¶è¿”å›æ™®é€š LLM
        print(f"âš ï¸  è­¦å‘Š: å½“å‰åç«¯ä¸æ”¯æŒ structured_outputï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿè§£æè·¯å¾„ã€‚é”™è¯¯: {e}")
        return base_llm

